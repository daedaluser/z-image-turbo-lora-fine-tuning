{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62090062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version is >= 2.5.0, check pass.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 20:33:18.649969: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-29 20:33:18.706437: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-29 20:33:19.680450: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[32m2026-01-29 20:33:20.314\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mutils.helpers\u001b[0m:\u001b[36mensure_model_weights\u001b[0m:\u001b[36m175\u001b[0m - \u001b[33m\u001b[1mManifest file not found: /home/liaoge/z-image/manifest.txt\u001b[0m\n",
      "\u001b[32m2026-01-29 20:33:20.315\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mutils.helpers\u001b[0m:\u001b[36mensure_model_weights\u001b[0m:\u001b[36m176\u001b[0m - \u001b[33m\u001b[1mSkipping file verification (assuming model exists)\u001b[0m\n",
      "\u001b[32m2026-01-29 20:33:20.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.helpers\u001b[0m:\u001b[36mensure_model_weights\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1m✓ Model directory exists: /home/liaoge/z-image\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen device:cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5e8adc824d4c7cafc64626d983a92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 LoRA 权重: /home/liaoge/Z-Image/src/zimage_lora_output_1/checkpoint-1200\n",
      "trainable params: 0 || all params: 6,236,009,536 || trainable%: 0.0000\n",
      "Available attention backends list: ['flash', 'flash_varlen', '_flash_3', '_flash_varlen_3', 'native', '_native_flash', '_native_math']\n",
      "Chosen attention backend: _native_flash\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Z-Image PyTorch Native Inference.\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from utils import AttentionBackend, ensure_model_weights, load_from_local_dir, set_attention_backend\n",
    "from zimage import generate\n",
    "from peft import PeftModel  # 新增导入\n",
    "\n",
    "\n",
    "\n",
    "model_path = ensure_model_weights(\"/home/xxx/z-image\", verify=False)  # True to verify with md5\n",
    "dtype = torch.bfloat16\n",
    "compile = False  # default False for compatibility\n",
    "out_path = \"/home/xxx/Z-Image/pic_test\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "output_path = out_path+\"/test1.png\"\n",
    "height = 1024\n",
    "width = 1024\n",
    "num_inference_steps = 8\n",
    "guidance_scale = 0.0\n",
    "\n",
    "attn_backend = os.environ.get(\"ZIMAGE_ATTENTION\", \"_native_flash\")\n",
    "# attn_backend = os.environ.get(\"ZIMAGE_ATTENTION\", \"flash\")\n",
    "# Available attention backends list: ['flash', 'flash_varlen', '_flash_3', '_flash_varlen_3', 'native', '_native_flash', '_native_math']\n",
    "#这段代码中的 set_attention_backend 是一个配置函数\n",
    "#用于为你的 AI 模型选择不同的注意力计算后端。简单理解就是：它决定了模型在计算注意力时使用的\"算法引擎\"。\n",
    "# | 后端名称              | 含义                 | 适用场景                     |\n",
    "# | ----------------- | ------------------ | ------------------------ |\n",
    "# | `\"_native_flash\"` | 原生 Flash Attention | 通用，性能较好                  |\n",
    "# | `\"flash\"`         | Flash Attention 2  | 需要手动安装 flash-attn 库      |\n",
    "# | `\"_flash_3\"`      | Flash Attention 3  | 仅支持 Hopper 架构 GPU (H100) |\n",
    "# | `\"sdpa\"`          | PyTorch SDPA 默认    | 兼容性最好，无需额外安装             |\n",
    "# | `\"xformers\"`      | xFormers 实现        | 需要安装 xformers 库          |\n",
    "\n",
    "# prompt = (\n",
    "#     \"Young Chinese woman in red Hanfu, intricate embroidery. Impeccable makeup, red floral forehead pattern. \"\n",
    "#     \"Elaborate high bun, golden phoenix headdress, red flowers, beads. Holds round folding fan with lady, trees, bird. \"\n",
    "#     \"Neon lightning-bolt lamp (⚡️), bright yellow glow, above extended left palm. Soft-lit outdoor night background, \"\n",
    "#     \"silhouetted tiered pagoda (西安大雁塔), blurred colorful distant lights.\"\n",
    "# )\n",
    "device = \"cuda:0\"\n",
    "print(\"Chosen device:\"+device)\n",
    "\n",
    "# Load models\n",
    "components = load_from_local_dir(model_path, device=device, dtype=dtype, compile=compile)\n",
    "#================== 新增：LoRA 加载代码 ==================\n",
    "lora_path = \"\"  # 修改为你的 LoRA 路径\n",
    "\n",
    "print(f\"正在加载 LoRA 权重: {lora_path}\")\n",
    "\n",
    "# 方法A：不合并权重（推荐，支持多 LoRA 切换和权重调节）\n",
    "components[\"transformer\"] = PeftModel.from_pretrained(\n",
    "    components[\"transformer\"],\n",
    "    lora_path,\n",
    "    torch_dtype=dtype,  # 保持与基础模型一致 (bfloat16)\n",
    "    device_map=None,    # 因为我们已经手动移动到 device\n",
    ")\n",
    "\n",
    "# 可选：如果有多个 LoRA 想切换，可以设置活动适配器\n",
    "# components[\"transformer\"].set_adapter(\"default\")  # \"default\" 是默认名称\n",
    "\n",
    "# 方法B：合并权重（推理速度更快，但无法继续训练或调节权重）\n",
    "# components[\"transformer\"] = PeftModel.from_pretrained(\n",
    "#     components[\"transformer\"], \n",
    "#     lora_path,\n",
    "#     torch_dtype=dtype,\n",
    "# )\n",
    "# components[\"transformer\"] = components[\"transformer\"].merge_and_unload()  # 合并后不再是 PeftModel\n",
    "# print(\"✅ LoRA 权重已合并到基础模型\")\n",
    "\n",
    "# 确保模型在评估模式（重要！）\n",
    "components[\"transformer\"].eval()\n",
    "\n",
    "# 验证可训练参数（应显示 trainable params: 0，因为推理时冻结）\n",
    "if hasattr(components[\"transformer\"], \"print_trainable_parameters\"):\n",
    "    components[\"transformer\"].print_trainable_parameters()\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "AttentionBackend.print_available_backends()\n",
    "set_attention_backend(attn_backend)\n",
    "print(f\"Chosen attention backend: {attn_backend}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1b3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 20:33:28.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mGenerating image: 1024x1024, steps=8, cfg=0.0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 20:33:28.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mSampling loop start: 8 steps\u001b[0m\n",
      "Denoising: 100%|██████████| 8/8 [00:04<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-decode latent range: [-12.52, 10.09]\n",
      "Scaling factor: 0.3611\n",
      "Shift factor: 0.1159\n",
      "Time taken: 5.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 20:33:34.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mGenerating image: 1024x1024, steps=8, cfg=0.0\u001b[0m\n",
      "\u001b[32m2026-01-29 20:33:34.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mSampling loop start: 8 steps\u001b[0m\n",
      "Denoising: 100%|██████████| 8/8 [00:04<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-decode latent range: [-13.08, 9.40]\n",
      "Scaling factor: 0.3611\n",
      "Shift factor: 0.1159\n",
      "Time taken: 4.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 20:33:40.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mGenerating image: 1024x1024, steps=8, cfg=0.0\u001b[0m\n",
      "\u001b[32m2026-01-29 20:33:40.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mSampling loop start: 8 steps\u001b[0m\n",
      "Denoising: 100%|██████████| 8/8 [00:04<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-decode latent range: [-13.70, 9.22]\n",
      "Scaling factor: 0.3611\n",
      "Shift factor: 0.1159\n",
      "Time taken: 4.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 20:33:45.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mGenerating image: 1024x1024, steps=8, cfg=0.0\u001b[0m\n",
      "\u001b[32m2026-01-29 20:33:45.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mSampling loop start: 8 steps\u001b[0m\n",
      "Denoising: 100%|██████████| 8/8 [00:04<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-decode latent range: [-12.49, 9.41]\n",
      "Scaling factor: 0.3611\n",
      "Shift factor: 0.1159\n",
      "Time taken: 4.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 20:33:50.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mGenerating image: 1024x1024, steps=8, cfg=0.0\u001b[0m\n",
      "\u001b[32m2026-01-29 20:33:50.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mSampling loop start: 8 steps\u001b[0m\n",
      "Denoising: 100%|██████████| 8/8 [00:04<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-decode latent range: [-12.91, 9.64]\n",
      "Scaling factor: 0.3611\n",
      "Shift factor: 0.1159\n",
      "Time taken: 4.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 20:33:56.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mGenerating image: 1024x1024, steps=8, cfg=0.0\u001b[0m\n",
      "\u001b[32m2026-01-29 20:33:56.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mSampling loop start: 8 steps\u001b[0m\n",
      "Denoising: 100%|██████████| 8/8 [00:04<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-decode latent range: [-12.77, 9.26]\n",
      "Scaling factor: 0.3611\n",
      "Shift factor: 0.1159\n",
      "Time taken: 4.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 20:34:01.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mGenerating image: 1024x1024, steps=8, cfg=0.0\u001b[0m\n",
      "\u001b[32m2026-01-29 20:34:01.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mSampling loop start: 8 steps\u001b[0m\n",
      "Denoising: 100%|██████████| 8/8 [00:04<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-decode latent range: [-12.41, 10.06]\n",
      "Scaling factor: 0.3611\n",
      "Shift factor: 0.1159\n",
      "Time taken: 4.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 20:34:07.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mGenerating image: 1024x1024, steps=8, cfg=0.0\u001b[0m\n",
      "\u001b[32m2026-01-29 20:34:07.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mSampling loop start: 8 steps\u001b[0m\n",
      "Denoising: 100%|██████████| 8/8 [00:04<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-decode latent range: [-12.73, 9.72]\n",
      "Scaling factor: 0.3611\n",
      "Shift factor: 0.1159\n",
      "Time taken: 4.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 20:34:12.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mGenerating image: 1024x1024, steps=8, cfg=0.0\u001b[0m\n",
      "\u001b[32m2026-01-29 20:34:12.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mSampling loop start: 8 steps\u001b[0m\n",
      "Denoising: 100%|██████████| 8/8 [00:04<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-decode latent range: [-12.51, 8.85]\n",
      "Scaling factor: 0.3611\n",
      "Shift factor: 0.1159\n",
      "Time taken: 4.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 20:34:17.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mGenerating image: 1024x1024, steps=8, cfg=0.0\u001b[0m\n",
      "\u001b[32m2026-01-29 20:34:18.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzimage.pipeline\u001b[0m:\u001b[36mgenerate\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mSampling loop start: 8 steps\u001b[0m\n",
      "Denoising: 100%|██████████| 8/8 [00:04<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-decode latent range: [-12.11, 9.08]\n",
      "Scaling factor: 0.3611\n",
      "Shift factor: 0.1159\n",
      "Time taken: 4.81 seconds\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    prompt = (\n",
    "    \"\"\n",
    ")\n",
    "   \n",
    "    output_path = out_path+f\"/qc{i}.png\"\n",
    "    seed = random.randint(43, 10000)\n",
    "        \n",
    "    # Gen an image\n",
    "    start_time = time.time()\n",
    "    images = generate(\n",
    "        prompt=prompt,\n",
    "        **components,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=torch.Generator(device).manual_seed(seed),\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "    images[0].save(output_path)\n",
    "\n",
    "### !! For best speed performance, recommend to use `_flash_3` backend and set `compile=True`\n",
    "### This would give you sub-second generation speed on Hopper GPU (H100/H200/H800) after warm-up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f4e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zimage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
