{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537d0845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Z-Image LoRA Fine-tuning Script (Fixed)\n",
    "åŸºäº Flow Matching 8æ­¥æ‰©æ•£çš„å®Œæ•´è®­ç»ƒå®ç°\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# å‡è®¾è¿™äº›æ˜¯ä½ çš„é¡¹ç›®å¯¼å…¥\n",
    "from utils.loader import load_from_local_dir\n",
    "from zimage.pipeline import generate, calculate_shift\n",
    "from config import DEFAULT_MAX_SEQUENCE_LENGTH\n",
    "\n",
    "# å…³é”®ï¼šè®¾ç½®æ˜¾å­˜åˆ†é…ç­–ç•¥ï¼Œå‡å°‘ç¢ç‰‡åŒ–\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,garbage_collection_threshold:0.6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f47e75da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    å›¾åƒ-æ–‡æœ¬å¯¹æ•°æ®é›†\n",
    "    æ”¯æŒå¤šåˆ†è¾¨ç‡è®­ç»ƒ\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir: str, resolution: int = 1024):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.resolution = resolution\n",
    "        \n",
    "        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡\n",
    "        self.image_paths = []\n",
    "        for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.webp\"]:\n",
    "            self.image_paths.extend(list(self.data_dir.glob(ext)))\n",
    "        \n",
    "        # åŠ è½½å¯¹åº”æ–‡æœ¬æè¿°\n",
    "        self.captions = {}\n",
    "        for img_path in self.image_paths:\n",
    "            txt_path = img_path.with_suffix(\".txt\")\n",
    "            if txt_path.exists():\n",
    "                with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    self.captions[img_path] = f.read().strip()\n",
    "            else:\n",
    "                # å¦‚æœæ²¡æœ‰txtï¼Œä½¿ç”¨å›¾ç‰‡åï¼ˆå»æ‰æ‰©å±•åï¼‰ä½œä¸ºæç¤ºè¯\n",
    "                self.captions[img_path] = img_path.stem.replace(\"_\", \" \")\n",
    "        \n",
    "        # å›¾åƒé¢„å¤„ç†ï¼ˆä¸æ¨ç†ä¸€è‡´ï¼Œå½’ä¸€åŒ–åˆ°[-1, 1]ï¼‰\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(resolution),\n",
    "            transforms.ToTensor(),  # [0, 1]\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),  # [-1, 1]\n",
    "        ])\n",
    "        \n",
    "        print(f\"åŠ è½½äº† {len(self.image_paths)} å¼ è®­ç»ƒå›¾ç‰‡\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"åŠ è½½å›¾ç‰‡å¤±è´¥ {image_path}: {e}\")\n",
    "            # è¿”å›ç©ºç™½å›¾ä½œä¸ºfallback\n",
    "            image = torch.zeros(3, self.resolution, self.resolution)\n",
    "        \n",
    "        caption = self.captions[image_path]\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"caption\": caption,\n",
    "        }\n",
    "\n",
    "\n",
    "class ZImageLoRATrainer:\n",
    "    \"\"\"\n",
    "    Z-image LoRA è®­ç»ƒå™¨\n",
    "    å®Œå…¨å¤ç”¨æ¨ç†ä»£ç ä¸­çš„ Scheduler å’Œç»„ä»¶\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        components: Dict,\n",
    "        device: str = \"cuda\",\n",
    "        dtype: torch.dtype = torch.bfloat16,\n",
    "        lora_rank: int = 64,\n",
    "        lora_alpha: int = 128,\n",
    "        lora_dropout: float = 0.0,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # è§£åŒ…ç»„ä»¶ï¼ˆå®Œå…¨å¤ç”¨ loader.py åŠ è½½çš„ç»„ä»¶ï¼‰\n",
    "        self.transformer = components[\"transformer\"]\n",
    "        self.vae = components[\"vae\"]\n",
    "        self.text_encoder = components[\"text_encoder\"]\n",
    "        self.tokenizer = components[\"tokenizer\"]\n",
    "        self.scheduler = components[\"scheduler\"]  # å…³é”®ï¼šå¤ç”¨ FlowMatchEulerDiscreteScheduler\n",
    "        \n",
    "        vae_config = self.vae.config\n",
    "        self.vae_scaling_factor = getattr(vae_config, 'scaling_factor', 0.3611)\n",
    "        self.vae_shift_factor = getattr(vae_config, 'shift_factor', 0.1159)\n",
    "    \n",
    "        print(f\"âœ… VAE Config - Scaling: {self.vae_scaling_factor}, Shift: {self.vae_shift_factor}\")\n",
    "        print(f\"   Latent ç›®æ ‡èŒƒå›´: ~[-10, 10] (åŒ¹é…æ¨ç†)\")\n",
    "\n",
    "        # å†»ç»“åŸºç¡€æ¨¡å‹ï¼ˆåªè®­ç»ƒ LoRAï¼‰\n",
    "        self.vae.requires_grad_(False)\n",
    "        self.text_encoder.requires_grad_(False)\n",
    "        self.transformer.requires_grad_(False)\n",
    "        \n",
    "        # é…ç½® LoRA\n",
    "        target_modules = self._detect_target_modules()\n",
    "        print(f\"æ£€æµ‹åˆ° {len(target_modules)} ä¸ª LoRA ç›®æ ‡æ¨¡å—\")\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=target_modules,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            init_lora_weights=\"gaussian\",\n",
    "        )\n",
    "\n",
    "        # åº”ç”¨ LoRA åˆ° Transformer\n",
    "        self.transformer = get_peft_model(self.transformer, lora_config)\n",
    "        self.transformer.to(device, dtype=dtype)\n",
    "        \n",
    "        # æ‰“å°å¯è®­ç»ƒå‚æ•°\n",
    "        self.transformer.print_trainable_parameters()\n",
    "        \n",
    "        # è®¾ç½®è®­ç»ƒæ¨¡å¼\n",
    "        self.transformer.train()\n",
    "        self.vae.eval()\n",
    "        self.text_encoder.eval()\n",
    "    \n",
    "    def _detect_target_modules(self) -> List[str]:\n",
    "        \"\"\"åªé€‰æ‹© Transformer Block å†…éƒ¨çš„ Attention å’Œ FFNï¼Œä¸¥æ ¼æ’é™¤æ‰€æœ‰è¾“å…¥/è¾“å‡ºå±‚\"\"\"\n",
    "        target_modules = []\n",
    "        \n",
    "        for name, module in self.transformer.named_modules():\n",
    "            # ä¸¥æ ¼æ’é™¤è§„åˆ™ï¼ˆåŒ…å«è¿™äº›å…³é”®è¯çš„éƒ½è·³è¿‡ï¼‰\n",
    "            if any(bad in name for bad in [\"all_x_embedder\", \"all_final_layer\", \"x_pad_token\", \"cap_pad_token\", \n",
    "                                        \"cap_embedder\", \"t_embedder\", \"noise_refiner\", \"context_refiner\"]):\n",
    "                continue\n",
    "                \n",
    "            # åªåŒ…å«è§„åˆ™ï¼šAttention å’Œ FFN\n",
    "            # Z-Image ä½¿ç”¨ to_q, to_k, to_v, to_out.0 ä½œä¸º attention æŠ•å½±\n",
    "            # ä½¿ç”¨ w1, w2, w3 ä½œä¸º FFN å±‚ (å‚è§ FeedForward ç±»)\n",
    "            # if any(good in name for good in [\"to_q\", \"to_k\", \"to_v\", \"to_out.0\", \"w1\", \"w2\", \"w3\"]):\n",
    "            if any(good in name for good in [\"w1\", \"w2\", \"w3\"]):\n",
    "            # if any(good in name for good in [\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"]):\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    target_modules.append(name)\n",
    "        \n",
    "        unique_modules = list(dict.fromkeys(target_modules))\n",
    "        print(f\"æ£€æµ‹åˆ° {len(unique_modules)} ä¸ª LoRA ç›®æ ‡æ¨¡å—ï¼ˆå·²æ’é™¤æ‰€æœ‰ embedderï¼‰\")\n",
    "        for name in unique_modules[:5]:  # æ‰“å°å‰5ä¸ªä½œä¸ºç¤ºä¾‹\n",
    "            print(f\"  - {name}\")\n",
    "        return unique_modules\n",
    "    \n",
    "    def encode_prompts(self, captions: List[str], max_length: int = 512) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        ç¼–ç æ–‡æœ¬æç¤ºï¼ˆä¸æ¨ç†ä»£ç å®Œå…¨ä¸€è‡´ï¼‰\n",
    "        è¿”å› List[Tensor] ä»¥æ”¯æŒå˜é•¿åºåˆ—\n",
    "        \"\"\"\n",
    "        # ä½¿ç”¨ chat templateï¼ˆä¸æ¨ç†ä¸€è‡´ï¼‰\n",
    "        formatted_prompts = []\n",
    "        for caption in captions:\n",
    "            messages = [{\"role\": \"user\", \"content\": caption}]\n",
    "            formatted = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=True,\n",
    "            )\n",
    "            formatted_prompts.append(formatted)\n",
    "        \n",
    "        # Tokenizeï¼ˆå›ºå®šé•¿åº¦å¡«å……ï¼‰\n",
    "        text_inputs = self.tokenizer(\n",
    "            formatted_prompts,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # ç¼–ç ï¼ˆå†»ç»“ï¼‰\n",
    "        with torch.no_grad():\n",
    "            outputs = self.text_encoder(\n",
    "                input_ids=text_inputs.input_ids,\n",
    "                attention_mask=text_inputs.attention_mask,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "            # å–å€’æ•°ç¬¬äºŒå±‚ï¼ˆä¸æ¨ç†ä¸€è‡´ï¼‰\n",
    "            prompt_embeds = outputs.hidden_states[-2]\n",
    "        \n",
    "        # è£å‰ª paddingï¼Œè½¬æ¢ä¸º List[Tensor]\n",
    "        prompt_embeds_list = []\n",
    "        masks = text_inputs.attention_mask.bool()\n",
    "        for i in range(len(prompt_embeds)):\n",
    "            # åªä¿ç•™çœŸå® tokenï¼ˆå»æ‰ paddingï¼‰\n",
    "            valid_embeds = prompt_embeds[i][masks[i]]\n",
    "            prompt_embeds_list.append(valid_embeds)\n",
    "        \n",
    "        return prompt_embeds_list\n",
    "    \n",
    "    def get_training_timesteps(self, batch_size: int, image_seq_len: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆè®­ç»ƒæ—¶é—´æ­¥ï¼ˆå¤ç”¨ scheduler çš„ shift é€»è¾‘ï¼‰\n",
    "        è¿”å› timesteps åœ¨ [0, 1] èŒƒå›´å†…ï¼Œå…¶ä¸­ 0=clean, 1=noise\n",
    "        \"\"\"\n",
    "        # è®¡ç®— muï¼ˆä¸æ¨ç†ä»£ç  calculate_shift ä¸€è‡´ï¼‰\n",
    "        base_seq_len = 256\n",
    "        max_seq_len = 4096\n",
    "        base_shift = 0.5\n",
    "        max_shift = 1.15\n",
    "        \n",
    "        m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n",
    "        b = base_shift - m * base_seq_len\n",
    "        mu = image_seq_len * m + b\n",
    "        \n",
    "        # ç”Ÿæˆéšæœºæ—¶é—´æ­¥ [0, 1]\n",
    "        # è®­ç»ƒæ—¶å‡åŒ€é‡‡æ ·ï¼Œè®©æ¨¡å‹çœ‹åˆ°å„ç§ t\n",
    "        t_raw = torch.rand(batch_size, device=self.device)\n",
    "        \n",
    "        # åº”ç”¨ time_shiftï¼ˆä¸ scheduler ä¸€è‡´ï¼‰\n",
    "        # ä½¿ç”¨ scheduler.time_shift å¦‚æœå¯ç”¨ï¼Œå¦åˆ™æ‰‹åŠ¨å®ç°\n",
    "        if hasattr(self.scheduler, 'time_shift'):\n",
    "            t_shifted = self.scheduler.time_shift(mu, 1.0, t_raw)\n",
    "        else:\n",
    "            # æ‰‹åŠ¨å®ç°ï¼ˆä¸ FlowMatchEulerDiscreteScheduler ä¸€è‡´ï¼‰\n",
    "            import math\n",
    "            t_shifted = math.exp(mu) / (math.exp(mu) + (1 / t_raw - 1) ** 1.0)\n",
    "        \n",
    "        # æ³¨æ„ï¼šåœ¨ Flow Matching ä¸­ï¼Œé€šå¸¸ t=0 æ˜¯å¹²å‡€æ•°æ®ï¼Œt=1 æ˜¯çº¯å™ªå£°\n",
    "        # ä½† Z-Image çš„å®ç°å¯èƒ½éœ€è¦ç‰¹å®šæ–¹å‘ï¼Œè¿™é‡Œä¿æŒä¸åŸæ¥ä¸€è‡´\n",
    "        timesteps = 1.0 - t_shifted\n",
    "        \n",
    "        return timesteps, mu\n",
    "    \n",
    "    def add_noise(self, latents: torch.Tensor, noise: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Flow Matching å‰å‘åŠ å™ªï¼šx_t = (1-t) * x_0 + t * noise\n",
    "        æ”¯æŒ batch å¤„ç†\n",
    "        \"\"\"\n",
    "        # t: [B] -> [B, 1, 1, 1] ç”¨äºå¹¿æ’­\n",
    "        t = t.view(-1, 1, 1, 1)\n",
    "        return (1 - t) * latents + t * noise\n",
    "    \n",
    "    def encode_vae(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        images_fp32 = images.to(dtype=torch.float32)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            # 1. VAE Encoder è¾“å‡º (raw latent, é€šå¸¸æ˜¯ [-20, 20] èŒƒå›´)\n",
    "            h = self.vae.encoder(images_fp32)\n",
    "            mean, logvar = torch.chunk(h, 2, dim=1)\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            latents = mean + eps * std  # é‡‡æ ·\n",
    "            \n",
    "            # 2. å…³é”®ä¿®å¤ï¼šåº”ç”¨ä¸æ¨ç†å®Œå…¨ç›¸åŒçš„é¢„å¤„ç†\n",
    "            # generate.py ä¸­: latents = (latents / scaling) + shift\n",
    "            latents = (latents / self.vae_scaling_factor) + self.vae_shift_factor\n",
    "            \n",
    "            # ç°åœ¨ latents èŒƒå›´åº”è¯¥åœ¨ [-8, 8] é™„è¿‘ï¼Œä¸æ¨ç†ä¸€è‡´\n",
    "            return latents.to(dtype=self.dtype)\n",
    "    \n",
    "    def compute_loss(self, latents, noise, timesteps, prompt_embeds_list):\n",
    "        \"\"\"\n",
    "        è®¡ç®— Flow Matching æŸå¤±\n",
    "        latents: [B, C, H, W] - VAE ç¼–ç åçš„å¹²å‡€ latent\n",
    "        noise: [B, C, H, W] - æ ‡å‡†é«˜æ–¯å™ªå£°\n",
    "        timesteps: [B] - æ—¶é—´æ­¥\n",
    "        prompt_embeds_list: List[Tensor] - æ–‡æœ¬ç¼–ç åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        # åŠ å™ª\n",
    "        noisy_latents = self.add_noise(latents, noise, timesteps)\n",
    "        \n",
    "        # è½¬æ¢ä¸º List æ ¼å¼ä»¥åŒ¹é… Transformer è¾“å…¥\n",
    "        # Transformer æœŸæœ› List[[C, F, H, W]]ï¼Œå…¶ä¸­ F æ˜¯å¸§æ•°ï¼ˆå›¾ç‰‡ä¸º1ï¼‰\n",
    "        latent_list = [noisy_latents[i] for i in range(latents.shape[0])]\n",
    "        # æ·»åŠ å¸§ç»´åº¦ [C, H, W] -> [C, 1, H, W]\n",
    "        latent_list = [x.unsqueeze(1) for x in latent_list]\n",
    "        \n",
    "        # Transformer å‰å‘\n",
    "        # æ³¨æ„ï¼šZ-Image ä½¿ç”¨ bfloat16/float16 è®­ç»ƒï¼Œéœ€è¦ autocast\n",
    "        with torch.cuda.amp.autocast(dtype=self.dtype):\n",
    "            model_output = self.transformer(\n",
    "                latent_list,\n",
    "                timesteps,\n",
    "                prompt_embeds_list,\n",
    "            )[0]  # è¿”å› (output, {})ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ \n",
    "        \n",
    "        # å¤„ç†è¾“å‡ºï¼šmodel_output æ˜¯ List[Tensor]\n",
    "        # æ¯ä¸ª Tensor å½¢çŠ¶æ˜¯ [C, F, H, W]ï¼ŒF=1\n",
    "        if isinstance(model_output, (list, tuple)):\n",
    "            # å †å ä¸º [B, C, F, H, W]\n",
    "            velocity_pred = torch.stack([x for x in model_output], dim=0)\n",
    "        else:\n",
    "            velocity_pred = model_output\n",
    "        \n",
    "        # å»æ‰å¸§ç»´åº¦ -> [B, C, H, W]\n",
    "        if velocity_pred.dim() == 5:\n",
    "            velocity_pred = velocity_pred.squeeze(2)\n",
    "        \n",
    "        # Flow Matching ç›®æ ‡ï¼šv = noise - x_0\n",
    "        velocity_target = noise - latents\n",
    "        \n",
    "        # MSE æŸå¤±\n",
    "        loss = F.mse_loss(velocity_pred.float(), velocity_target.float(), reduction=\"mean\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "def train_lora(\n",
    "    model_path: str,\n",
    "    train_data_dir: str,\n",
    "    output_dir: str = \"./zimage_lora_output\",\n",
    "    num_train_epochs: int = 10,\n",
    "    batch_size: int = 1,\n",
    "    learning_rate: float = 1e-4,\n",
    "    gradient_accumulation_steps: int = 4,\n",
    "    save_steps: int = 500,\n",
    "    resolution: int = 896,\n",
    "    device: str = \"cuda\",\n",
    "    dtype: torch.dtype = torch.bfloat16,\n",
    "    lora_rank: int = 64,\n",
    "    lora_alpha: int = 128,\n",
    "):\n",
    "    \"\"\"\n",
    "    ä¸»è®­ç»ƒå‡½æ•°\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"æ­£åœ¨åŠ è½½æ¨¡å‹ä»: {model_path}\")\n",
    "    \n",
    "    # è¿™é‡Œéœ€è¦ä½ å®é™…çš„åŠ è½½å‡½æ•°\n",
    "    components = load_from_local_dir(\n",
    "        model_path,\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # éªŒè¯ VAE å’Œ Transformer ç»´åº¦åŒ¹é…ï¼ˆåœ¨åˆ›å»º trainer ä¹‹å‰ï¼‰\n",
    "    print(\"=== ç»´åº¦éªŒè¯ ===\")\n",
    "    if hasattr(components[\"vae\"], \"encoder\"):\n",
    "        # æµ‹è¯• VAE è¾“å‡ºç»´åº¦\n",
    "        test_img = torch.zeros(1, 3, resolution, resolution).to(device=device, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            test_out = components[\"vae\"].encoder(test_img)\n",
    "            actual_c = test_out.shape[1] // 2  # å› ä¸º double_z=Trueï¼Œè¾“å‡ºæ˜¯ 2*latent_channels\n",
    "            \n",
    "            expected_c = components[\"transformer\"].in_channels\n",
    "            \n",
    "            if actual_c != expected_c:\n",
    "                print(f\"âŒ é”™è¯¯: VAE latent channels={actual_c} â‰  Transformer in_channels={expected_c}\")\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"âœ… ç»´åº¦åŒ¹é…: latent_channels={actual_c}\")\n",
    "    \n",
    "    # åˆå§‹åŒ–è®­ç»ƒå™¨ï¼ˆè¿™æ—¶æ‰åˆ›å»º trainerï¼‰\n",
    "    trainer = ZImageLoRATrainer(\n",
    "        components=components,\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "        lora_rank=lora_rank,\n",
    "        lora_alpha=lora_alpha,\n",
    "    )\n",
    "    \n",
    "    # å‡†å¤‡æ•°æ®é›†\n",
    "    dataset = ImageTextDataset(train_data_dir, resolution=resolution)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    # ä¼˜åŒ–å™¨ï¼ˆåªä¼˜åŒ– LoRA å‚æ•°ï¼‰\n",
    "    optimizer = AdamW(\n",
    "        filter(lambda p: p.requires_grad, trainer.transformer.parameters()),\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    \n",
    "    # å­¦ä¹ ç‡è°ƒåº¦\n",
    "    total_steps = len(dataloader) * num_train_epochs // gradient_accumulation_steps\n",
    "    lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=max(1, total_steps//10), T_mult=2, eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    print(f\"å¼€å§‹è®­ç»ƒï¼Œæ€»æ­¥æ•°: {total_steps}ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}ï¼Œæ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps}\")\n",
    "    \n",
    "    global_step = 0\n",
    "    trainer.transformer.train()\n",
    "    \n",
    "    # è·å– scaling factor\n",
    "    # vae_scaling_factor = getattr(trainer.vae.config, 'scaling_factor', 0.18215)\n",
    "    \n",
    "    for epoch in range(num_train_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_train_epochs}\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for step, batch in enumerate(pbar):\n",
    "            images = batch[\"image\"].to(device=device)\n",
    "            captions = batch[\"caption\"]\n",
    "            \n",
    "            # 1. VAE ç¼–ç ï¼ˆå†»ç»“ï¼‰\n",
    "            with torch.no_grad():\n",
    "                latents = trainer.encode_vae(images)\n",
    "                \n",
    "                # åº”ç”¨ scaling factorï¼ˆä¸æ¨ç†ä¸€è‡´ï¼‰\n",
    "                latents = latents * trainer.vae_scaling_factor\n",
    "                \n",
    "                # è®¡ç®— image_seq_len ç”¨äº muï¼ˆè€ƒè™‘ patchifyï¼‰\n",
    "                # latent å½¢çŠ¶: [B, C, H, W]\n",
    "                b, c, h, w = latents.shape\n",
    "                # å‡è®¾ä½¿ç”¨ 2x2 patchï¼ˆä» all_patch_size=(2,) æ¨æ–­ï¼‰\n",
    "                patch_size = 2\n",
    "                image_seq_len = (h // patch_size) * (w // patch_size)\n",
    "                \n",
    "                # æ–‡æœ¬ç¼–ç ï¼ˆå†»ç»“ï¼‰\n",
    "                prompt_embeds_list = trainer.encode_prompts(captions)\n",
    "            \n",
    "            # 2. ç”Ÿæˆå™ªå£°\n",
    "            noise = torch.randn_like(latents)\n",
    "            \n",
    "            # 3. ç”Ÿæˆæ—¶é—´æ­¥ï¼ˆå¤ç”¨ scheduler é€»è¾‘ï¼‰\n",
    "            timesteps, mu = trainer.get_training_timesteps(\n",
    "                batch_size=latents.shape[0],\n",
    "                image_seq_len=image_seq_len,\n",
    "            )\n",
    "            \n",
    "            # 4. è®¡ç®—æŸå¤±ï¼ˆå¸¦æ¢¯åº¦ç´¯ç§¯ï¼‰\n",
    "            with torch.cuda.amp.autocast(dtype=dtype):\n",
    "                loss = trainer.compute_loss(latents, noise, timesteps, prompt_embeds_list)\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            epoch_loss += loss.item() * gradient_accumulation_steps\n",
    "            \n",
    "            # 5. æ¢¯åº¦æ›´æ–°\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(trainer.transformer.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    \"loss\": f\"{loss.item() * gradient_accumulation_steps:.4f}\",\n",
    "                    \"lr\": f\"{lr_scheduler.get_last_lr()[0]:.2e}\",\n",
    "                    \"mu\": f\"{mu:.2f}\",\n",
    "                })\n",
    "                \n",
    "                # ä¿å­˜æ£€æŸ¥ç‚¹\n",
    "                if global_step % save_steps == 0:\n",
    "                    checkpoint_dir = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n",
    "                    trainer.transformer.save_pretrained(checkpoint_dir)\n",
    "                    \n",
    "                    # ä¿å­˜è®­ç»ƒé…ç½®\n",
    "                    config = {\n",
    "                        \"lora_rank\": lora_rank,\n",
    "                        \"lora_alpha\": lora_alpha,\n",
    "                        \"resolution\": resolution,\n",
    "                        \"global_step\": global_step,\n",
    "                        \"mu\": mu,\n",
    "                    }\n",
    "                    with open(os.path.join(checkpoint_dir, \"train_config.json\"), \"w\") as f:\n",
    "                        json.dump(config, f, indent=2)\n",
    "                    \n",
    "                    print(f\"\\nâœ… ä¿å­˜æ£€æŸ¥ç‚¹åˆ°: {checkpoint_dir}\")\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} å®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_loss:.4f}\")\n",
    "    \n",
    "    # ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "    final_dir = os.path.join(output_dir, \"final_lora\")\n",
    "    trainer.transformer.save_pretrained(final_dir)\n",
    "    print(f\"ğŸ‰ è®­ç»ƒå®Œæˆï¼LoRA æƒé‡ä¿å­˜è‡³: {final_dir}\")\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "\n",
    "def load_lora_for_inference(components: Dict, lora_path: str, device: str = \"cuda\", merge: bool = False):\n",
    "    \"\"\"\n",
    "    æ¨ç†æ—¶åŠ è½½ LoRA æƒé‡\n",
    "    \n",
    "    Args:\n",
    "        components: åŸºç¡€æ¨¡å‹ç»„ä»¶\n",
    "        lora_path: LoRA æƒé‡è·¯å¾„\n",
    "        device: è®¡ç®—è®¾å¤‡\n",
    "        merge: æ˜¯å¦åˆå¹¶æƒé‡ï¼ˆåˆå¹¶åæ— æ³•ç»§ç»­è®­ç»ƒï¼Œä½†æ¨ç†æ›´å¿«ï¼‰\n",
    "    \"\"\"\n",
    "    transformer = PeftModel.from_pretrained(\n",
    "        components[\"transformer\"],\n",
    "        lora_path,\n",
    "    )\n",
    "    \n",
    "    if merge:\n",
    "        transformer = transformer.merge_and_unload()\n",
    "        print(\"âœ… LoRA æƒé‡å·²åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹\")\n",
    "    \n",
    "    components[\"transformer\"] = transformer.to(device)\n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6da447",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/home/xxx/z-image\"  # ä½ çš„æ¨¡å‹è·¯å¾„\n",
    "TRAIN_DATA_DIR = \"/home/xxx/pic_spider/downloaded_images\"  # è®­ç»ƒæ•°æ®ç›®å½•\n",
    "OUTPUT_DIR = \"./zimage_lora_output_1\"\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "trainer = train_lora(\n",
    "    model_path=MODEL_PATH,\n",
    "    train_data_dir=TRAIN_DATA_DIR,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=15,\n",
    "    batch_size=1,  # æ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼Œå»ºè®®ä»1å¼€å§‹æµ‹è¯•\n",
    "    learning_rate=1e-5,\n",
    "    gradient_accumulation_steps=4,  # æœ‰æ•ˆ batch_size = 4\n",
    "    save_steps=100,\n",
    "    resolution=768,\n",
    "    device=\"cuda:0\",\n",
    "    dtype=torch.bfloat16,\n",
    "    lora_rank=64,   # å¯è°ƒæ•´ä¸º 16/32/64/128\n",
    "    lora_alpha=128, # é€šå¸¸è®¾ä¸º 2*rank\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38950e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zimage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
